import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from peft import PeftModel
from datasets import load_dataset, interleave_datasets
from transformers import Trainer, TrainingArguments
from time import sleep
import json

# –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º
BASE_MODEL_PATH = r"D:/Unlocked_Model"  # –ü–∞–ø–∫–∞ —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
LORA_ADAPTER_PATH = r"D:/MyFreedomGPT-LoRA".replace("\\", "/")  # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
MERGED_MODEL_PATH = r"D:/MyFreedomGPT-Merged"  # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é)
print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞)...")
base_model = None
for attempt in range(3):  # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –±–µ–∑ –≤–µ—Å–æ–≤
        config = AutoConfig.from_pretrained(BASE_MODEL_PATH)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
        base_model = AutoModelForCausalLM.from_config(config)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –±–µ–∑ –≤–µ—Å–æ–≤
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        break  # –í—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–∞ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
    except RuntimeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        if attempt < 2:
            print("üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
            sleep(5)
        else:
            print("‚ö†Ô∏è –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–µ —É–≤–µ–Ω—á–∞–ª–∏—Å—å —É—Å–ø–µ—Ö–æ–º. –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É.")

# –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤—ã–≤–æ–¥–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –∑–∞–≤–µ—Ä—à–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
if base_model is None:
    print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è.")
    exit(1)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
device = "cpu"
print("üìç –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π")
base_model.to(device)  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –Ω–∞ CPU

# 4Ô∏è‚É£ **–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã**
print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã...")

dataset_urls = {
    "code": "bigcode/the-stack",
    "news": "cc_news",
    "pile": "EleutherAI/pile",
    "openwebtext": "openwebtext",  # –û—Å—Ç–∞–≤–ª—è–µ–º OpenWebText
    "daily_dialog": "daily_dialog"  # –î–æ–±–∞–≤–ª—è–µ–º DailyDialog –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
}

datasets = []
for name, url in dataset_urls.items():
    try:
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º {name} (—Ä–µ–∂–∏–º —Å—Ç—Ä–∏–º–∏–Ω–≥–∞)...")
        ds = load_dataset(url, split="train", streaming=True, trust_remote_code=True)
        ds = ds.take(50)  # üî• –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 –ø—Ä–∏–º–µ—Ä–æ–≤ (—É–º–µ–Ω—å—à–∞–µ–º –ø–∞–º—è—Ç—å)
        datasets.append(ds)
        print(f"‚úÖ {name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {name}: {e}")

# ‚úÖ **–û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã**
if datasets:
    dataset = interleave_datasets(datasets, stopping_strategy="first_exhausted")
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
else:
    dataset = None
    print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç.")

# 5Ô∏è‚É£ **–ù–∞—Å—Ç—Ä–æ–∏–º LoRA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è**
print("üîß –ù–∞—Å—Ç—Ä–æ–∏–º LoRA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
adapter_config_path = os.path.join(LORA_ADAPTER_PATH, 'adapter_config.json')
if not os.path.exists(adapter_config_path):
    print("‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π...")

    # –°–æ–∑–¥–∞—ë–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
    adapter_config = {
        "adapter_type": "LoRA",  # –¢–∏–ø –∞–¥–∞–ø—Ç–µ—Ä–∞
        "base_model": BASE_MODEL_PATH,  # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
        "r": 8,  # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –¥–ª—è LoRA (–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏)
        "lora_alpha": 16,  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç LoRA (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
        "lora_dropout": 0.1,  # Dropout –¥–ª—è LoRA (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
        "trainable": True,  # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA –±—É–¥—É—Ç –æ–±—É—á–∞–µ–º—ã–º–∏
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    os.makedirs(LORA_ADAPTER_PATH, exist_ok=True)
    with open(adapter_config_path, 'w') as f:
        json.dump(adapter_config, f, indent=4)
    print(f"‚úÖ –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å–æ–∑–¥–∞–Ω –ø–æ –ø—É—Ç–∏: {adapter_config_path}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
try:
    print(f"üîß –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä –∏–∑ {LORA_ADAPTER_PATH}...")
    lora_adapter = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
    print("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞: {e}")
    exit(1)

# 6Ô∏è‚É£ **–û–±—É—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞**
print("üìö –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞...")

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º tokenization –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
def preprocess_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
train_dataset = dataset.map(preprocess_function, batched=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./results",          # –ü–∞–ø–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    evaluation_strategy="epoch",     # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç–ø–æ–∫–∞
    learning_rate=2e-5,              # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    per_device_train_batch_size=4,   # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —É–º–µ–Ω—å—à–µ–Ω –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø–∞–º—è—Ç—å
    gradient_accumulation_steps=4,   # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ (–∏–º–∏—Ç–∞—Ü–∏—è –±–æ–ª—å—à–µ–≥–æ –±–∞—Ç—á–∞)
    num_train_epochs=3,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    weight_decay=0.01,               # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    logging_dir="./logs",            # –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤
    fp16=True,                       # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    load_best_model_at_end=True,     # –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è
)

# –û–±—É—á–∞—é—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å
trainer = Trainer(
    model=lora_adapter,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)

# –û–±—É—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
trainer.train()

# 7Ô∏è‚É£ **–°–ª–∏—è–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å –º–æ–¥–µ–ª—å—é**
print("üîÑ –°–ª–∏–≤–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä —Å –º–æ–¥–µ–ª—å—é...")
merged_model = lora_adapter.merge_and_unload()  # –°–ª–∏—è–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å –º–æ–¥–µ–ª—å—é

# 8Ô∏è‚É£ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏**
if not os.path.exists(MERGED_MODEL_PATH):
    os.makedirs(MERGED_MODEL_PATH)  # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
merged_model.save_pretrained(MERGED_MODEL_PATH)
tokenizer.save_pretrained(MERGED_MODEL_PATH)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MERGED_MODEL_PATH}")
