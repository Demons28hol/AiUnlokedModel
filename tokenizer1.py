import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from peft import PeftModel
from datasets import load_dataset, interleave_datasets
from transformers import Trainer, TrainingArguments
from time import sleep
import shutil
from google.colab import files
import json

# –£–∫–∞–∑—ã–≤–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face
BASE_MODEL_REPO_URL = "https://huggingface.co/MisterHolY/Unloked_Model-Mistral.7-B"  # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–≤–æ—é –º–æ–¥–µ–ª—å Hugging Face
MERGED_MODEL_PATH = "/content/merged_model"  # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ Hugging Face
print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è Hugging Face...")
base_model = None
for attempt in range(3):  # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
        config = AutoConfig.from_pretrained(BASE_MODEL_REPO_URL)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
        base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_REPO_URL, config=config)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –≤–µ—Å–∞–º–∏
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        break
    except RuntimeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        if attempt < 2:
            print("üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
            sleep(5)
        else:
            print("‚ö†Ô∏è –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–µ —É–≤–µ–Ω—á–∞–ª–∏—Å—å —É—Å–ø–µ—Ö–æ–º. –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É.")

# –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∑–∞–≤–µ—Ä—à–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
if base_model is None:
    print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è.")
    exit(1)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
device = "cpu"
base_model.to(device)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã...")
dataset_urls = {
    "code": "bigcode/the-stack",
    "news": "cc_news",
    "pile": "EleutherAI/pile",
    "openwebtext": "openwebtext",
    "daily_dialog": "daily_dialog"
}
datasets = []
for name, url in dataset_urls.items():
    try:
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º {name} (—Ä–µ–∂–∏–º —Å—Ç—Ä–∏–º–∏–Ω–≥–∞)...")
        ds = load_dataset(url, split="train", streaming=True, trust_remote_code=True)
        ds = ds.take(50)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 –ø—Ä–∏–º–µ—Ä–æ–≤
        datasets.append(ds)
        print(f"‚úÖ {name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {name}: {e}")

if datasets:
    dataset = interleave_datasets(datasets, stopping_strategy="first_exhausted")
    print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
else:
    dataset = None
    print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç.")

# –ù–∞—Å—Ç—Ä–æ–∏–º LoRA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
print("üîß –ù–∞—Å—Ç—Ä–æ–∏–º LoRA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
LORA_ADAPTER_PATH = "/content/lora_adapter"  # –ü—É—Ç—å –¥–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
adapter_config_path = os.path.join(LORA_ADAPTER_PATH, 'adapter_config.json')
if not os.path.exists(adapter_config_path):
    adapter_config = {
        "adapter_type": "LoRA",
        "base_model": BASE_MODEL_REPO_URL,  # –ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        "r": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.1,
        "trainable": True,
    }
    os.makedirs(LORA_ADAPTER_PATH, exist_ok=True)
    with open(adapter_config_path, 'w') as f:
        json.dump(adapter_config, f, indent=4)

# –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
try:
    print("üîß –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä...")
    lora_adapter = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
    print("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞: {e}")
    exit(1)

# –û–±—É—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
print("üìö –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_REPO_URL)
def preprocess_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)
train_dataset = dataset.map(preprocess_function, batched=True)

training_args = TrainingArguments(
    output_dir="./results", 
    evaluation_strategy="epoch", 
    learning_rate=2e-5, 
    per_device_train_batch_size=4, 
    gradient_accumulation_steps=4, 
    num_train_epochs=3,
    weight_decay=0.01, 
    logging_dir="./logs",
    fp16=True,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=lora_adapter, 
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)

trainer.train()

# –°–ª–∏—è–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å –º–æ–¥–µ–ª—å—é
print("üîÑ –°–ª–∏–≤–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä —Å –º–æ–¥–µ–ª—å—é...")
merged_model = lora_adapter.merge_and_unload()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å
print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
if not os.path.exists(MERGED_MODEL_PATH):
    os.makedirs(MERGED_MODEL_PATH)

merged_model.save_pretrained(MERGED_MODEL_PATH)
tokenizer.save_pretrained(MERGED_MODEL_PATH)

# –°–æ–∑–¥–∞—ë–º –∞—Ä—Ö–∏–≤
shutil.make_archive(MERGED_MODEL_PATH, 'zip', MERGED_MODEL_PATH)

# –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤
print("üì• –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤ –º–æ–¥–µ–ª–∏...")
files.download(f'{MERGED_MODEL_PATH}.zip')


